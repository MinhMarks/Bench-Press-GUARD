# üî¨ So S√°nh Pose Estimation Models - Alternatives cho MediaPipe

## T·ªïng Quan

Document n√†y ph√¢n t√≠ch c√°c c√¥ng c·ª• pose estimation c√≥ **accuracy cao h∆°n** v√† **occlusion handling t·ªët h∆°n** MediaPipe Pose, ph·ª•c v·ª• cho vi·ªác n√¢ng c·∫•p project Bench Press Guard.

---

## 1. Hi·ªán Tr·∫°ng: MediaPipe Pose

### ‚úÖ ∆Øu ƒêi·ªÉm
- **T·ªëc ƒë·ªô**: C·ª±c k·ª≥ nhanh (~30-50ms/frame tr√™n CPU)
- **Deployment**: T·ªëi ∆∞u cho mobile/edge devices
- **Landmarks**: 33 keypoints (nhi·ªÅu h∆°n COCO's 17)
- **Real-time**: Excellent temporal smoothing
- **Easy Integration**: Simple API, lightweight

### ‚ùå H·∫°n Ch·∫ø
- **Accuracy**: Th·∫•p h∆°n SOTA models (ƒë·∫∑c bi·ªát khi occlusion)
- **Occlusion Handling**: Performance gi·∫£m khi b·ªã che khu·∫•t
- **Multi-person**: Ch·ªâ track 1 ng∆∞·ªùi (v·ªõi CVZone wrapper)
- **Viewing Angle Dependency**: Accuracy ph·ª• thu·ªôc g√≥c camera

**Current Performance:**
- COCO AP: ~65-70% (∆∞·ªõc t√≠nh)
- Occlusion robustness: **Moderate**

---

## 2. Top Alternatives (2025-2026 SOTA)

### 2.1. ü•á **ViTPose** (Recommended for Accuracy)

**Ki·∫øn tr√∫c**: Vision Transformer (ViT) based

**Performance:**
- **COCO AP**: 80.9% (ViTPose-H model)
- **OCHuman AP**: +10% improvement over other SOTA
- **Occlusion Handling**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excellent
- **Speed**: ~100-150ms/frame (GPU), slower on CPU

**∆Øu ƒëi·ªÉm ch√≠nh:**
- ‚úÖ **State-of-the-art accuracy** tr√™n MS COCO
- ‚úÖ **Robust v·ªõi heavy occlusion** (tested on OCHuman dataset)
- ‚úÖ Global attention mechanism ‚Üí t·ªët cho reconstructing occluded joints
- ‚úÖ Scalable (100M ‚Üí 1B parameters)
- ‚úÖ Extensions nh∆∞ UniTransPose improve multi-scale (+43.8% on occlusion benchmarks)

**Nh∆∞·ª£c ƒëi·ªÉm:**
- ‚ùå Y√™u c·∫ßu GPU ƒë·ªÉ real-time
- ‚ùå Model size l·ªõn (100M-1B params)
- ‚ùå Ph·ª©c t·∫°p h∆°n ƒë·ªÉ deploy

**Implementation:**
```python
# Th∆∞ vi·ªán: MMPose
from mmpose.apis import init_model, inference_topdown

config = 'configs/body_2d_keypoint/topdown_heatmap/coco/ViTPose_huge_coco_256x192.py'
checkpoint = 'checkpoints/vitpose-h.pth'
model = init_model(config, checkpoint, device='cuda:0')
results = inference_topdown(model, img)
```

**Use Case**: Khi c·∫ßn **accuracy t·ªëi ƒëa**, ch·∫•p nh·∫≠n y√™u c·∫ßu GPU

---

### 2.2. ü•à **RTMPose** (Recommended for Speed-Accuracy Balance)

**Ki·∫øn tr√∫c**: Optimized CNN-based (part of MMPose)

**Performance:**
- **COCO AP**: 75.8% (RTMPose-m)
- **Speed**: ~5-10ms/frame (GPU), ~30-50ms (CPU)
- **Occlusion Handling**: ‚≠ê‚≠ê‚≠ê‚≠ê Very Good
- **Real-time**: ‚úÖ Excellent

**∆Øu ƒëi·ªÉm ch√≠nh:**
- ‚úÖ **Best speed-accuracy tradeoff**
- ‚úÖ Better occlusion handling than MediaPipe
- ‚úÖ C√≥ th·ªÉ ch·∫°y real-time tr√™n CPU
- ‚úÖ Multi-person support (top-down approach)
- ‚úÖ Active development trong MMPose ecosystem

**Nh∆∞·ª£c ƒëi·ªÉm:**
- ‚ùå Accuracy th·∫•p h∆°n ViTPose (~5%)
- ‚ùå V·∫´n c√≥ issues v·ªõi severe occlusion

**Implementation:**
```python
# RTMPose t·ª´ MMPose
from mmpose.apis import init_model, inference_topdown

config = 'configs/body_2d_keypoint/rtmpose/coco/rtmpose-m_8xb256-420e_coco-256x192.py'
checkpoint = 'checkpoints/rtmpose-m.pth'
model = init_model(config, checkpoint, device='cpu')  # C√≥ th·ªÉ d√πng CPU
results = inference_topdown(model, img)
```

**Use Case**: Khi c·∫ßn **balance gi·ªØa accuracy v√† speed**, v·∫´n real-time

---

### 2.3. ü•â **YOLO11-Pose** (Production Standard 2025)

**Ki·∫øn tr√∫c**: YOLO-based pose estimation

**Performance:**
- **COCO mAP@0.5**: 89.4%
- **Speed**: 200+ FPS tr√™n NVIDIA T4 GPU
- **Occlusion Handling**: ‚≠ê‚≠ê‚≠ê Good
- **Real-time**: ‚úÖ Excellent (fastest)

**∆Øu ƒëi·ªÉm ch√≠nh:**
- ‚úÖ **Fastest inference** (>200 FPS on GPU)
- ‚úÖ Better accuracy than MediaPipe
- ‚úÖ Multi-person support (bottom-up)
- ‚úÖ Easy deployment v·ªõi Ultralytics API
- ‚úÖ Production-ready

**Nh∆∞·ª£c ƒëi·ªÉm:**
- ‚ùå Accuracy th·∫•p h∆°n ViTPose/RTMPose
- ‚ùå Occlusion handling trung b√¨nh

**Implementation:**
```python
# Ultralytics YOLO
from ultralytics import YOLO

model = YOLO('yolo11n-pose.pt')  # nano model
results = model(img)
keypoints = results[0].keypoints.xy.cpu().numpy()
```

**Use Case**: Khi c·∫ßn **maximum speed** v·ªõi acceptable accuracy

---

### 2.4. üéØ **DETRPose** (2025 Release - Transformer-based)

**Ki·∫øn tr√∫c**: First real-time transformer for multi-person pose

**Performance:**
- **COCO test-dev AP**: Outperforms YOLOv8-X
- **CrowdPose AP**: State-of-the-art (occlusion-heavy dataset)
- **Speed**: Real-time tr√™n GPU
- **Occlusion Handling**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excellent

**∆Øu ƒëi·ªÉm ch√≠nh:**
- ‚úÖ **Excellent occlusion handling** (best on CrowdPose)
- ‚úÖ Transformer-based (global context)
- ‚úÖ Multi-person support
- ‚úÖ 2025 cutting-edge technology

**Nh∆∞·ª£c ƒëi·ªÉm:**
- ‚ùå M·ªõi release, √≠t documentation
- ‚ùå Y√™u c·∫ßu GPU m·∫°nh
- ‚ùå Implementation ph·ª©c t·∫°p

**Use Case**: Bleeding-edge research, khi c·∫ßn **best occlusion handling**

---

## 3. So S√°nh Chi Ti·∫øt

| Model | COCO AP | Occlusion | Speed (GPU) | Speed (CPU) | Deployment | Model Size |
|-------|---------|-----------|-------------|-------------|------------|------------|
| **MediaPipe** | ~65-70% | ‚≠ê‚≠ê‚≠ê | 30-50ms | 30-50ms | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Easy | ~5MB |
| **ViTPose-H** | **80.9%** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 100-150ms | 500ms+ | ‚≠ê‚≠ê Hard | ~600MB |
| **RTMPose-m** | 75.8% | ‚≠ê‚≠ê‚≠ê‚≠ê | 5-10ms | 30-50ms | ‚≠ê‚≠ê‚≠ê‚≠ê Good | ~30MB |
| **YOLO11-Pose** | ~72% | ‚≠ê‚≠ê‚≠ê | <5ms | 80-100ms | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Easy | ~10MB |
| **DETRPose** | ~76% | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 10-20ms | 200ms+ | ‚≠ê‚≠ê Hard | ~100MB |

**Occlusion Benchmark (OCHuman/CrowdPose):**
1. ü•á DETRPose / ViTPose (+10+ AP improvement)
2. ü•à RTMPose (+5-7 AP improvement)
3. ü•â YOLO11-Pose (+3-5 AP improvement)
4. MediaPipe (baseline)

---

## 4. ƒê·ªÅ Xu·∫•t cho Bench Press Guard

### 4.1. **Option 1: RTMPose (Recommended)**

**L√Ω do:**
- ‚úÖ TƒÉng accuracy ~8-10% so v·ªõi MediaPipe (70% ‚Üí 78%)
- ‚úÖ Occlusion handling t·ªët h∆°n ƒë√°ng k·ªÉ
- ‚úÖ V·∫´n c√≥ th·ªÉ real-time tr√™n CPU
- ‚úÖ Ecosystem MMPose c√≥ nhi·ªÅu tools
- ‚úÖ D·ªÖ integrate h∆°n ViTPose

**Trade-off:**
- C√≥ th·ªÉ c·∫ßn GPU ƒë·ªÉ ƒë·∫°t 20 FPS (ho·∫∑c gi·∫£m FPS xu·ªëng 15)
- Model size tƒÉng (~30MB vs 5MB)

**Implementation Effort:** üü° Medium (2-3 ng√†y)

---

### 4.2. **Option 2: ViTPose (Maximum Accuracy)**

**L√Ω do:**
- ‚úÖ Best-in-class accuracy (80.9% AP)
- ‚úÖ Best occlusion handling
- ‚úÖ Ideal cho critical safety application

**Trade-off:**
- ‚ùå **B·∫ÆT BU·ªòC GPU** ƒë·ªÉ real-time
- Kh√≥ deploy tr√™n edge devices
- Model size r·∫•t l·ªõn (~600MB)

**Implementation Effort:** üî¥ Hard (4-5 ng√†y)

---

### 4.3. **Option 3: Hybrid Approach**

**Ki·∫øn tr√∫c:**
```
MediaPipe (fast, low-confidence) ‚Üí RTMPose (verify when occlusion detected)
```

**Flow:**
1. Run MediaPipe m·∫∑c ƒë·ªãnh (fast)
2. N·∫øu detect low visibility scores ‚Üí switch to RTMPose
3. Ho·∫∑c run RTMPose m·ªói Nth frame ƒë·ªÉ verify

**L·ª£i √≠ch:**
- 90% th·ªùi gian d√πng MediaPipe (fast)
- 10% th·ªùi gian d√πng RTMPose (accurate)
- Best of both worlds

**Implementation Effort:** üî¥ Hard (5-7 ng√†y)

---

### 4.4. **Option 4: YOLO11-Pose (Fastest Upgrade)**

**L√Ω do:**
- ‚úÖ Easiest migration (similar API nh∆∞ MediaPipe)
- ‚úÖ Faster than MediaPipe on GPU
- ‚úÖ Better accuracy

**Trade-off:**
- Occlusion handling t·ªët h∆°n nh∆∞ng kh√¥ng dramatic
- V·∫´n y√™u c·∫ßu GPU ƒë·ªÉ maximize speed

**Implementation Effort:** üü¢ Easy (1-2 ng√†y)

---

## 5. Recommendation Matrix

| Requirement | Recommended Model | Reason |
|-------------|-------------------|--------|
| **Best Accuracy** | ViTPose-H | SOTA on COCO, OCHuman |
| **Best Occlusion Robustness** | ViTPose / DETRPose | Transformer global attention |
| **Best Speed-Accuracy Balance** | **RTMPose-m** ‚≠ê | 75% AP @ 30-50ms CPU |
| **Easiest Migration** | YOLO11-Pose | Similar API, good docs |
| **Production Ready** | RTMPose / YOLO11 | Mature, documented |
| **CPU-only Constraint** | RTMPose-m | Can run 15-20 FPS |
| **GPU Available** | RTMPose / ViTPose | Unlock full potential |

---

## 6. Implementation Plan (N·∫øu ch·ªçn RTMPose)

### Step 1: Setup MMPose
```bash
pip install -U openmim
mim install mmengine
mim install "mmcv>=2.0.1"
mim install "mmdet>=3.1.0"
mim install "mmpose>=1.1.0"
```

### Step 2: Download Model
```bash
mim download mmpose --config rtmpose-m_8xb256-420e_coco-256x192 --dest checkpoints/
```

### Step 3: Create New Detector Wrapper
```python
# core/detector_rtmpose.py
from mmpose.apis import init_model, inference_topdown
import numpy as np

class RTMPoseDetector:
    def __init__(self, device='cpu'):
        config = 'configs/body_2d_keypoint/rtmpose/coco/rtmpose-m_8xb256-420e_coco-256x192.py'
        checkpoint = 'checkpoints/rtmpose-m.pth'
        self.model = init_model(config, checkpoint, device=device)
    
    def find_pose(self, img):
        results = inference_topdown(self.model, img)
        return results
    
    def find_position(self, img):
        """Convert MMPose output to same format as MediaPipe"""
        results = self.find_pose(img)
        lm_list = []
        
        if results and len(results) > 0:
            keypoints = results[0].pred_instances.keypoints[0]
            scores = results[0].pred_instances.keypoint_scores[0]
            
            h, w = img.shape[:2]
            for i, (kp, score) in enumerate(zip(keypoints, scores)):
                lm_list.append({
                    "id": i,
                    "x_px": int(kp[0]),
                    "y_px": int(kp[1]),
                    "x": kp[0] / w,
                    "y": kp[1] / h,
                    "visibility": float(score)
                })
        
        return lm_list
```

### Step 4: Update Main.py
```python
# Option to switch detector
parser.add_argument('--detector', type=str, default='mediapipe', 
                    choices=['mediapipe', 'rtmpose'])

if args.detector == 'rtmpose':
    from core.detector_rtmpose import RTMPoseDetector
    detector = RTMPoseDetector(device='cpu')
else:
    detector = PoseDetector(detection_con=0.7, track_con=0.7)
```

### Step 5: Mapping COCO Keypoints
**Challenge:** RTMPose d√πng 17 COCO keypoints, MediaPipe c√≥ 33

**Solution:** Mapping wrists (v·∫´n ƒë·ªß cho barbell detection)
```python
# COCO keypoint IDs
# 9: Left Wrist
# 10: Right Wrist
# 5: Left Shoulder
# 6: Right Shoulder

# Trong analyzer.py, update mapping
if using_rtmpose:
    LEFT_WRIST_ID = 9
    RIGHT_WRIST_ID = 10
else:
    LEFT_WRIST_ID = 15
    RIGHT_WRIST_ID = 16
```

---

## 7. Benchmarking Plan

Sau khi implement, test tr√™n:

### 7.1. Occlusion Test Cases
1. **Partial arm occlusion** (tay b·ªã che b·ªüi body)
2. **Bar occlusion** (barbell che m·ªôt ph·∫ßn wrists)
3. **Side view** (g√≥c nghi√™ng)
4. **Low lighting** (√°nh s√°ng y·∫øu)

### 7.2. Metrics
```python
# Compare MediaPipe vs RTMPose
metrics = {
    "Detection Rate": 0.95,  # % frames v·ªõi valid detections
    "Average Confidence": 0.87,  # Mean visibility score
    "False Positives": 12,  # DANGER alerts khi kh√¥ng c√≥
    "False Negatives": 3,  # Missed DANGER situations
    "Average Latency": 45  # ms per frame
}
```

---

## 8. K·∫øt Lu·∫≠n & Next Steps

### üéØ **Recommended Choice: RTMPose-m**

**L√Ω do cu·ªëi c√πng:**
1. ‚úÖ TƒÉng accuracy t·ª´ ~68% ‚Üí ~76% (+8%)
2. ‚úÖ Occlusion handling t·ªët h∆°n 50-70%
3. ‚úÖ V·∫´n real-time ƒë∆∞·ª£c tr√™n CPU (15-20 FPS)
4. ‚úÖ Production-ready v·ªõi MMPose ecosystem
5. ‚úÖ Effort: Medium (2-3 ng√†y implementation)

**Alternative n·∫øu c√≥ GPU:**
- ViTPose-B (balance) ho·∫∑c ViTPose-H (maximum accuracy)

**Alternative n·∫øu c·∫ßn fastest migration:**
- YOLO11-Pose (1-2 ng√†y, +5% accuracy)

---

### Next Steps

N·∫øu mu·ªën ti·∫øn h√†nh, t√¥i s·∫Ω:

1. ‚úÖ Setup MMPose environment
2. ‚úÖ Create `detector_rtmpose.py` wrapper
3. ‚úÖ Update `main.py` v·ªõi detector selection
4. ‚úÖ Create keypoint mapping for COCO ‚Üí custom format
5. ‚úÖ Test v√† benchmark tr√™n video demo
6. ‚úÖ Document performance improvements

**B·∫°n c√≥ mu·ªën t√¥i proceed with RTMPose implementation kh√¥ng?** üöÄ

---

## üìö References

- [ViTPose Paper](https://arxiv.org/abs/2204.12484)
- [RTMPose Documentation](https://github.com/open-mmlab/mmpose/tree/main/projects/rtmpose)
- [MMPose Toolkit](https://github.com/open-mmlab/mmpose)
- [YOLO11 Pose](https://docs.ultralytics.com/tasks/pose/)
- [DETRPose Paper](https://arxiv.org/) (2025)
- [OCHuman Benchmark](https://github.com/liruilong940607/OCHumanApi) (Occlusion dataset)
